"""
Migration Monitoring System for MCP Safety - NMSTX-257

Provides real-time monitoring, progress tracking, and safety validation
for the NMSTX-253 MCP System Refactor with automatic alerting and rollback triggers.

This module extracts and enhances the monitoring capabilities from migrate_mcp_system.py
with production-ready safety features and comprehensive metrics tracking.
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum

from ..config.feature_flags import get_feature_flags, MigrationMode

logger = logging.getLogger(__name__)


class MonitoringLevel(Enum):
    """Monitoring intensity levels."""
    MINIMAL = "minimal"      # Basic error tracking
    STANDARD = "standard"    # Standard monitoring with performance
    INTENSIVE = "intensive"  # Full monitoring with detailed metrics
    DEBUG = "debug"         # Debug-level monitoring with traces


class AlertSeverity(Enum):
    """Alert severity levels."""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class PerformanceMetrics:
    """Performance metrics for migration monitoring."""
    response_time_ms: float = 0.0
    query_count: int = 0
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class AlertEvent:
    """Alert event data structure."""
    timestamp: datetime
    severity: AlertSeverity
    message: str
    context: str
    metrics: Optional[Dict[str, Any]] = None
    threshold_exceeded: Optional[str] = None


@dataclass
class SafetyThresholds:
    """Configurable safety thresholds for monitoring."""
    max_errors: int = 5
    max_warnings: int = 20
    max_duration_minutes: float = 30.0
    min_success_rate: float = 0.8
    max_response_time_ms: float = 5000.0
    max_memory_usage_mb: float = 1024.0
    max_cpu_usage_percent: float = 80.0
    performance_degradation_threshold: float = 0.2  # 20% degradation


class MigrationMonitor:
    """
    Real-time migration monitoring with safety triggers and alerting.
    
    Provides comprehensive monitoring of migration progress, performance metrics,
    safety validation, and automatic rollback trigger capabilities.
    
    Features:
    - Real-time progress tracking with timestamps
    - Performance regression detection
    - Configurable safety thresholds
    - Alert system with severity levels
    - Automatic rollback trigger detection
    - Zero data loss verification
    - Memory and CPU monitoring
    """
    
    def __init__(self, 
                 monitoring_level: MonitoringLevel = MonitoringLevel.STANDARD,
                 custom_thresholds: Optional[SafetyThresholds] = None):
        """
        Initialize migration monitor.
        
        Args:
            monitoring_level: Intensity of monitoring
            custom_thresholds: Custom safety thresholds
        """
        self.monitoring_level = monitoring_level
        self.thresholds = custom_thresholds or SafetyThresholds()
        
        # Monitoring state
        self.start_time: Optional[datetime] = None
        self.last_checkpoint: Optional[datetime] = None
        self.is_monitoring: bool = False
        
        # Event tracking
        self.alerts: List[AlertEvent] = []
        self.errors: List[Dict[str, Any]] = []
        self.warnings: List[Dict[str, Any]] = []
        self.progress_events: List[Dict[str, Any]] = []
        
        # Performance tracking
        self.performance_history: List[PerformanceMetrics] = []
        self.baseline_metrics: Optional[PerformanceMetrics] = None
        
        # Statistics
        self.operations_total: int = 0
        self.operations_successful: int = 0
        self.operations_failed: int = 0
        
        # Callback handlers
        self.alert_handlers: List[Callable[[AlertEvent], None]] = []
        self.rollback_handlers: List[Callable[[str], None]] = []
        
        # Feature flag integration
        self.feature_flags = get_feature_flags()
        
        logger.info(f"ðŸ” Migration monitor initialized (level: {monitoring_level.value})")
    
    def add_alert_handler(self, handler: Callable[[AlertEvent], None]) -> None:
        """Add an alert event handler."""
        self.alert_handlers.append(handler)
    
    def add_rollback_handler(self, handler: Callable[[str], None]) -> None:
        """Add a rollback trigger handler."""
        self.rollback_handlers.append(handler)
    
    def start_monitoring(self, operation_name: str = "migration") -> None:
        """
        Start monitoring a migration operation.
        
        Args:
            operation_name: Name of the operation being monitored
        """
        if not self.feature_flags.is_enabled("MCP_MONITORING_ENABLED"):
            logger.info("Monitoring disabled by feature flag")
            return
        
        self.start_time = datetime.now()
        self.last_checkpoint = self.start_time
        self.is_monitoring = True
        
        # Clear previous session data
        self.alerts.clear()
        self.errors.clear()
        self.warnings.clear()
        self.progress_events.clear()
        self.performance_history.clear()
        
        # Reset counters
        self.operations_total = 0
        self.operations_successful = 0
        self.operations_failed = 0
        
        # Capture baseline metrics
        self.baseline_metrics = self._capture_performance_metrics()
        
        self._record_progress(f"Started monitoring {operation_name}", {"operation": operation_name})
        
        logger.info(f"ðŸ” Migration monitoring started for: {operation_name}")
    
    def stop_monitoring(self) -> Dict[str, Any]:
        """
        Stop monitoring and return final summary.
        
        Returns:
            Complete monitoring summary
        """
        if not self.is_monitoring:
            logger.warning("Monitoring was not active")
            return {}
        
        self.is_monitoring = False
        end_time = datetime.now()
        
        summary = self.get_monitoring_summary()
        summary["end_time"] = end_time.isoformat()
        summary["total_duration_seconds"] = (end_time - self.start_time).total_seconds()
        
        self._record_progress("Monitoring stopped", {"final_summary": summary})
        
        logger.info(f"ðŸ Migration monitoring stopped - Duration: {summary['total_duration_seconds']:.1f}s")
        return summary
    
    def record_operation_start(self, operation: str, context: Dict[str, Any] = None) -> str:
        """
        Record the start of an operation.
        
        Args:
            operation: Operation name
            context: Additional context
            
        Returns:
            Operation ID for tracking
        """
        if not self.is_monitoring:
            return ""
        
        operation_id = f"{operation}_{int(time.time() * 1000)}"
        self.operations_total += 1
        
        event_data = {
            "operation_id": operation_id,
            "operation": operation,
            "status": "started",
            "timestamp": datetime.now().isoformat(),
            "context": context or {}
        }
        
        self.progress_events.append(event_data)
        
        if self.monitoring_level in [MonitoringLevel.INTENSIVE, MonitoringLevel.DEBUG]:
            logger.debug(f"ðŸ” Operation started: {operation} (ID: {operation_id})")
        
        return operation_id
    
    def record_operation_success(self, operation_id: str, result: Dict[str, Any] = None) -> None:
        """
        Record successful completion of an operation.
        
        Args:
            operation_id: Operation ID from record_operation_start
            result: Operation result data
        """
        if not self.is_monitoring or not operation_id:
            return
        
        self.operations_successful += 1
        
        event_data = {
            "operation_id": operation_id,
            "status": "completed",
            "timestamp": datetime.now().isoformat(),
            "result": result or {}
        }
        
        self.progress_events.append(event_data)
        
        if self.monitoring_level in [MonitoringLevel.INTENSIVE, MonitoringLevel.DEBUG]:
            logger.debug(f"âœ… Operation completed: {operation_id}")
    
    def record_operation_failure(self, operation_id: str, error: str, context: str = "") -> None:
        """
        Record failure of an operation.
        
        Args:
            operation_id: Operation ID from record_operation_start
            error: Error message
            context: Additional error context
        """
        if not self.is_monitoring:
            return
        
        self.operations_failed += 1
        
        error_entry = {
            "operation_id": operation_id,
            "timestamp": datetime.now().isoformat(),
            "error": error,
            "context": context,
            "stack_trace": None  # Could add stack trace capture if needed
        }
        
        self.errors.append(error_entry)
        
        # Check if error threshold exceeded
        if len(self.errors) >= self.thresholds.max_errors:
            self._trigger_alert(
                AlertSeverity.CRITICAL,
                f"Error threshold exceeded: {len(self.errors)}/{self.thresholds.max_errors}",
                "error_threshold",
                {"error_count": len(self.errors), "threshold": self.thresholds.max_errors}
            )
        
        logger.error(f"âŒ Operation failed: {operation_id} - {error}")
    
    def record_warning(self, warning: str, context: str = "") -> None:
        """
        Record a warning event.
        
        Args:
            warning: Warning message
            context: Additional warning context
        """
        if not self.is_monitoring:
            return
        
        warning_entry = {
            "timestamp": datetime.now().isoformat(),
            "warning": warning,
            "context": context
        }
        
        self.warnings.append(warning_entry)
        
        # Check warning threshold
        if len(self.warnings) >= self.thresholds.max_warnings:
            self._trigger_alert(
                AlertSeverity.WARNING,
                f"Warning threshold exceeded: {len(self.warnings)}/{self.thresholds.max_warnings}",
                "warning_threshold",
                {"warning_count": len(self.warnings), "threshold": self.thresholds.max_warnings}
            )
        
        logger.warning(f"âš ï¸ Warning: {warning} (Context: {context})")
    
    def check_duration_threshold(self) -> bool:
        """
        Check if operation duration exceeds threshold.
        
        Returns:
            True if threshold exceeded
        """
        if not self.is_monitoring or not self.start_time:
            return False
        
        duration_minutes = (datetime.now() - self.start_time).total_seconds() / 60
        
        if duration_minutes > self.thresholds.max_duration_minutes:
            self._trigger_alert(
                AlertSeverity.ERROR,
                f"Duration threshold exceeded: {duration_minutes:.1f}/{self.thresholds.max_duration_minutes} minutes",
                "duration_threshold",
                {"duration_minutes": duration_minutes, "threshold": self.thresholds.max_duration_minutes}
            )
            return True
        
        return False
    
    async def monitor_performance(self) -> PerformanceMetrics:
        """
        Monitor current system performance.
        
        Returns:
            Current performance metrics
        """
        if not self.feature_flags.is_enabled("MCP_PERFORMANCE_MONITORING"):
            return PerformanceMetrics()
        
        current_metrics = self._capture_performance_metrics()
        self.performance_history.append(current_metrics)
        
        # Check for performance degradation
        if self.baseline_metrics:
            degradation = self._calculate_performance_degradation(current_metrics)
            
            if degradation > self.thresholds.performance_degradation_threshold:
                self._trigger_alert(
                    AlertSeverity.WARNING,
                    f"Performance degradation detected: {degradation:.1%}",
                    "performance_degradation",
                    {
                        "degradation_percent": degradation,
                        "current_response_time": current_metrics.response_time_ms,
                        "baseline_response_time": self.baseline_metrics.response_time_ms
                    }
                )
        
        # Check individual thresholds
        if current_metrics.response_time_ms > self.thresholds.max_response_time_ms:
            self._trigger_alert(
                AlertSeverity.WARNING,
                f"High response time: {current_metrics.response_time_ms:.1f}ms",
                "response_time_threshold",
                {"response_time_ms": current_metrics.response_time_ms, "threshold": self.thresholds.max_response_time_ms}
            )
        
        if current_metrics.memory_usage_mb > self.thresholds.max_memory_usage_mb:
            self._trigger_alert(
                AlertSeverity.WARNING,
                f"High memory usage: {current_metrics.memory_usage_mb:.1f}MB",
                "memory_threshold",
                {"memory_usage_mb": current_metrics.memory_usage_mb, "threshold": self.thresholds.max_memory_usage_mb}
            )
        
        return current_metrics
    
    def _capture_performance_metrics(self) -> PerformanceMetrics:
        """Capture current performance metrics."""
        try:
            import psutil
            process = psutil.Process()
            
            # Test database response time
            start_time = time.time()
            self._test_database_connectivity()
            response_time_ms = (time.time() - start_time) * 1000
            
            return PerformanceMetrics(
                response_time_ms=response_time_ms,
                memory_usage_mb=process.memory_info().rss / 1024 / 1024,
                cpu_usage_percent=process.cpu_percent(),
                query_count=len(self.progress_events),  # Proxy for query count
                timestamp=datetime.now()
            )
        except ImportError:
            logger.warning("psutil not available, using simplified metrics")
            # Fallback without psutil
            start_time = time.time()
            self._test_database_connectivity()
            response_time_ms = (time.time() - start_time) * 1000
            
            return PerformanceMetrics(
                response_time_ms=response_time_ms,
                timestamp=datetime.now()
            )
        except Exception as e:
            logger.warning(f"Error capturing performance metrics: {e}")
            return PerformanceMetrics(timestamp=datetime.now())
    
    def _test_database_connectivity(self) -> None:
        """Test database connectivity for response time measurement."""
        try:
            from src.db.connection import execute_query
            execute_query("SELECT 1", fetch=True)
        except Exception as e:
            logger.warning(f"Database connectivity test failed: {e}")
    
    def _calculate_performance_degradation(self, current: PerformanceMetrics) -> float:
        """Calculate performance degradation percentage."""
        if not self.baseline_metrics or self.baseline_metrics.response_time_ms == 0:
            return 0.0
        
        degradation = (current.response_time_ms - self.baseline_metrics.response_time_ms) / self.baseline_metrics.response_time_ms
        return max(0.0, degradation)  # Only positive degradation
    
    def _trigger_alert(self, severity: AlertSeverity, message: str, context: str, metrics: Dict[str, Any] = None) -> None:
        """Trigger an alert event."""
        alert = AlertEvent(
            timestamp=datetime.now(),
            severity=severity,
            message=message,
            context=context,
            metrics=metrics,
            threshold_exceeded=context if "threshold" in context else None
        )
        
        self.alerts.append(alert)
        
        # Call alert handlers
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                logger.error(f"Alert handler failed: {e}")
        
        # Check for rollback triggers
        if severity == AlertSeverity.CRITICAL and self.feature_flags.is_enabled("MCP_ENABLE_AUTO_ROLLBACK"):
            self._trigger_rollback(f"Critical alert: {message}")
    
    def _trigger_rollback(self, reason: str) -> None:
        """Trigger rollback handlers."""
        logger.critical(f"ðŸš¨ ROLLBACK TRIGGERED: {reason}")
        
        for handler in self.rollback_handlers:
            try:
                handler(reason)
            except Exception as e:
                logger.error(f"Rollback handler failed: {e}")
    
    def _record_progress(self, message: str, data: Dict[str, Any]) -> None:
        """Record a progress event."""
        event = {
            "timestamp": datetime.now().isoformat(),
            "message": message,
            "data": data,
            "checkpoint": len(self.progress_events)
        }
        
        self.progress_events.append(event)
        
        if self.monitoring_level in [MonitoringLevel.INTENSIVE, MonitoringLevel.DEBUG]:
            logger.debug(f"ðŸ“Š Progress: {message}")
    
    def get_success_rate(self) -> float:
        """Calculate current success rate."""
        if self.operations_total == 0:
            return 1.0
        return self.operations_successful / self.operations_total
    
    def get_error_rate(self) -> float:
        """Calculate current error rate."""
        if self.operations_total == 0:
            return 0.0
        return self.operations_failed / self.operations_total
    
    def is_healthy(self) -> bool:
        """
        Check if the migration is healthy based on all metrics.
        
        Returns:
            True if all health checks pass
        """
        if not self.is_monitoring:
            return True
        
        # Check error thresholds
        if len(self.errors) >= self.thresholds.max_errors:
            return False
        
        # Check success rate
        if self.get_success_rate() < self.thresholds.min_success_rate:
            return False
        
        # Check duration
        if self.check_duration_threshold():
            return False
        
        # Check for critical alerts
        critical_alerts = [alert for alert in self.alerts if alert.severity == AlertSeverity.CRITICAL]
        if critical_alerts:
            return False
        
        return True
    
    def get_monitoring_summary(self) -> Dict[str, Any]:
        """
        Get comprehensive monitoring summary.
        
        Returns:
            Complete monitoring summary with all metrics
        """
        current_time = datetime.now()
        duration_seconds = (current_time - self.start_time).total_seconds() if self.start_time else 0
        
        # Performance summary
        performance_summary = {}
        if self.performance_history:
            response_times = [m.response_time_ms for m in self.performance_history]
            performance_summary = {
                "avg_response_time_ms": sum(response_times) / len(response_times),
                "max_response_time_ms": max(response_times),
                "min_response_time_ms": min(response_times),
                "total_measurements": len(response_times)
            }
            
            if self.baseline_metrics:
                performance_summary["baseline_response_time_ms"] = self.baseline_metrics.response_time_ms
                performance_summary["performance_degradation"] = self._calculate_performance_degradation(self.performance_history[-1])
        
        # Alert summary
        alert_summary = {
            "total_alerts": len(self.alerts),
            "by_severity": {
                "critical": len([a for a in self.alerts if a.severity == AlertSeverity.CRITICAL]),
                "error": len([a for a in self.alerts if a.severity == AlertSeverity.ERROR]),
                "warning": len([a for a in self.alerts if a.severity == AlertSeverity.WARNING]),
                "info": len([a for a in self.alerts if a.severity == AlertSeverity.INFO])
            }
        }
        
        summary = {
            "monitoring_active": self.is_monitoring,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "current_time": current_time.isoformat(),
            "duration_seconds": duration_seconds,
            "duration_minutes": duration_seconds / 60,
            
            # Operation statistics
            "operations": {
                "total": self.operations_total,
                "successful": self.operations_successful,
                "failed": self.operations_failed,
                "success_rate": self.get_success_rate(),
                "error_rate": self.get_error_rate()
            },
            
            # Event counts
            "events": {
                "total_errors": len(self.errors),
                "total_warnings": len(self.warnings),
                "total_progress_events": len(self.progress_events)
            },
            
            # Health status
            "health": {
                "is_healthy": self.is_healthy(),
                "thresholds_breached": len([a for a in self.alerts if a.threshold_exceeded])
            },
            
            # Performance metrics
            "performance": performance_summary,
            
            # Alert summary
            "alerts": alert_summary,
            
            # Configuration
            "monitoring_level": self.monitoring_level.value,
            "feature_flags": {
                "monitoring_enabled": self.feature_flags.is_enabled("MCP_MONITORING_ENABLED"),
                "performance_monitoring": self.feature_flags.is_enabled("MCP_PERFORMANCE_MONITORING"),
                "auto_rollback": self.feature_flags.is_enabled("MCP_ENABLE_AUTO_ROLLBACK")
            }
        }
        
        return summary
    
    def export_detailed_report(self) -> Dict[str, Any]:
        """
        Export detailed monitoring report for analysis.
        
        Returns:
            Detailed report with all events and metrics
        """
        summary = self.get_monitoring_summary()
        
        # Add detailed event data
        summary["detailed_data"] = {
            "errors": self.errors,
            "warnings": self.warnings,
            "progress_events": self.progress_events,
            "alerts": [
                {
                    "timestamp": alert.timestamp.isoformat(),
                    "severity": alert.severity.value,
                    "message": alert.message,
                    "context": alert.context,
                    "metrics": alert.metrics,
                    "threshold_exceeded": alert.threshold_exceeded
                }
                for alert in self.alerts
            ],
            "performance_history": [
                {
                    "timestamp": metrics.timestamp.isoformat(),
                    "response_time_ms": metrics.response_time_ms,
                    "memory_usage_mb": metrics.memory_usage_mb,
                    "cpu_usage_percent": metrics.cpu_usage_percent,
                    "query_count": metrics.query_count
                }
                for metrics in self.performance_history
            ]
        }
        
        return summary